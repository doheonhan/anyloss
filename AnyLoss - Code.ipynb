{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b8176b0",
   "metadata": {},
   "source": [
    "# Here, the codes for our method (AnyLoss) is provided"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4479437",
   "metadata": {},
   "source": [
    "## 1. Code for Single-Layer Perceptron (SLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6f0cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# SLP with Mean Square Error (MSE) - Baseline 1 #################################\n",
    "def linear(X,theta):\n",
    "    z = np.dot(X,theta.T)\n",
    "    return z\n",
    "\n",
    "def cost_ada(z,y):\n",
    "    loss = ((y-z)**2).sum()\n",
    "    return loss/2.0\n",
    "\n",
    "def gd_ada(X,z,y):\n",
    "    return -np.dot((y-z), X)\n",
    "\n",
    "def update_loss(theta,learning_rate,gradient):\n",
    "    return theta-(learning_rate*gradient)\n",
    "\n",
    "def predict(X,theta):\n",
    "    outcome = []\n",
    "    result = linear(X,theta)\n",
    "    for i in range(X.shape[0]):\n",
    "        if result[i] <= 0:\n",
    "            outcome.append(-1)\n",
    "        else:\n",
    "            outcome.append(1)\n",
    "    return outcome \n",
    "\n",
    "\n",
    "theta = np.zeros(X_train_a.shape[1])   # Initialize theta with zeros\n",
    "num_iter = 1000\n",
    "cost = []\n",
    "lr = 0.1\n",
    "\n",
    "for i in range(num_iter):\n",
    "    z = linear(X_train_a,theta)\n",
    "    cost.append(cost_ada(z,y_train))\n",
    "    gradient = gd_ada(X_train_a,z,y_train)\n",
    "    theta = update_loss(theta,lr,gradient)\n",
    "outcome_r = predict(X_train_a,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968b4fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# SLP with Binary Cross-Entropy (BCE) - Baseline 2 #################################\n",
    "def sigmoid(X,theta):\n",
    "    z = np.dot(X,theta.T).astype(float)\n",
    "    return 1.0/(1+np.exp(-z))\n",
    "\n",
    "def cost_function(h,y):\n",
    "    loss = ((-y * np.log(h))-((1-y)* np.log(1-h))).mean()\n",
    "    return loss\n",
    "\n",
    "def gradient_descent(X,h,y,yl):\n",
    "    return np.dot(X.T,(h-y))/yl \n",
    "\n",
    "def update_loss(theta,learning_rate,gradient):\n",
    "    return theta-(learning_rate*gradient)\n",
    "\n",
    "def predict(X,theta):\n",
    "    outcome = []\n",
    "    result = sigmoid(X,theta)\n",
    "    for i in range(X.shape[0]):\n",
    "        if result[i] <= threshold:\n",
    "            outcome.append(0)\n",
    "        else:\n",
    "            outcome.append(1)\n",
    "    return outcome  \n",
    "\n",
    "\n",
    "theta = np.zeros(X_train_a.shape[1])   # Initialize theta with zeros\n",
    "threshold = 0.5\n",
    "num_iter = 1000\n",
    "cost = []\n",
    "lr = 0.1\n",
    "yl = y_train.shape[0]\n",
    "\n",
    "for i in range(num_iter):\n",
    "    h = sigmoid(X_train_a,theta)\n",
    "    cost.append(cost_function(h,y_train))\n",
    "    gradient = gradient_descent(X_train_a,h,y_train,yl)\n",
    "    theta = update_loss(theta,lr,gradient)\n",
    "outcome_r = predict(X_train_a,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36b9c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# SLP with AnyLoss (Ours) #################################\n",
    "def sigmoid(X,theta):\n",
    "    z = np.dot(X,theta.T).astype(float)\n",
    "    return 1.0/(1+np.exp(-z))\n",
    "\n",
    "def sigmoid_2(p):   # Approximation Function\n",
    "    s = (L*(p-threshold)).astype(float)\n",
    "    return 1.0/(1+np.exp(-s))\n",
    "\n",
    "############################# For AnyLoss ###############################\n",
    "\n",
    "############### For fbeta score ###############\n",
    "def cost_function_fbeta(bs,syh,syhy,bs_sy):\n",
    "    loss = (1+bs)*syhy / ( bs_sy + syh )    # fbeta score\n",
    "    return 1-loss\n",
    "\n",
    "def gradient_descent_fbeta(p,X,yh,y,bs,syh,syhy,bs_sy):\n",
    "    yp_pz = L*yh*(1-yh) * p*(1-p)\n",
    "    return -((1+bs)*(np.dot(y*yp_pz*(bs_sy+syh), X)-np.dot(yp_pz*syhy, X))) / ((bs_sy + syh)**2)  # derivative of fbeta\n",
    "############### For accuracy score ##############\n",
    "def cost_function_acc(sy,syh,syhy,yl):\n",
    "    loss = (yl-sy-syh+2*syhy)/yl   # accyracy\n",
    "    return 1-loss\n",
    "\n",
    "def gradient_descent_acc(p,X,yh,y,yl):   \n",
    "    yp_pz = L*yh*(1-yh) * p*(1-p)\n",
    "    return (np.dot(yp_pz, X) - 2*np.dot(y*yp_pz, X)) / yl    # derivative of accyracy\n",
    "############### For geometric mean ###############\n",
    "def cost_function_gmean(sy,syh,syhy,yl):\n",
    "    loss = (syhy*(yl-syh-sy+syhy)/(sy*(yl-sy)))**0.5  # gmean\n",
    "    return 1-loss\n",
    "\n",
    "def gradient_descent_gmean(p,X,yh,y,sy,syh,syhy,yl):\n",
    "    yp_pz = L*yh*(1-yh) * p*(1-p)\n",
    "    repeat1 = np.dot(y*yp_pz, X)\n",
    "    repeat2 = yl-syh-sy+syhy\n",
    "    return -2*((repeat1*repeat2)+(-np.dot(yp_pz, X)+repeat1)*syhy)/(sy*(yl-sy)*syhy*(repeat2))**0.5  # derivative of gmean\n",
    "############### For balanced accuracy ###############\n",
    "def cost_function_balacc(sy,syh,syhy,yl):\n",
    "    loss = (yl*(syhy+sy)-sy*(syh+sy)) / (2*sy*(yl-sy))     # balanced accuracy\n",
    "    return 1-loss\n",
    "\n",
    "def gradient_descent_balacc(p,X,yh,y,sy,yl):\n",
    "    yp_pz = L*yh*(1-yh) * p*(1-p)\n",
    "    return -(yl*np.dot(y*yp_pz, X)-sy*np.dot(yp_pz, X))/(2*sy*(yl-sy))  # derivative of balanced accuracy\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "def update_loss(theta,learning_rate,gradient):\n",
    "    return theta-(learning_rate*gradient)\n",
    "\n",
    "def predict(X,theta):\n",
    "    outcome = []\n",
    "    result = sigmoid(X,theta)\n",
    "    for i in range(X.shape[0]):\n",
    "        if result[i] <= threshold:\n",
    "            outcome.append(0)\n",
    "        else:\n",
    "            outcome.append(1)\n",
    "    return outcome  \n",
    "\n",
    "\n",
    "theta = np.zeros(X_train_a.shape[1])   # Initialize theta with zeros\n",
    "threshold = 0.5\n",
    "num_iter = 1000\n",
    "L = 73\n",
    "cost = []\n",
    "lr = 0.1\n",
    "sy = np.sum(y_train)\n",
    "yl = y_train.shape[0]\n",
    "\n",
    "for i in range(num_iter):\n",
    "    p = sigmoid(X_train_a,theta)\n",
    "    yh = sigmoid_2(p)           # Approximation (probablility -> like a label)\n",
    "    syh = np.sum(yh)\n",
    "    syhy = np.dot(yh,y_train)\n",
    "    # For 'cost' and 'gradient', you can choose any pair of defined functions above for a specific goal.\n",
    "    cost.append([   ])     # [   ]: cost_function_fbeta(bs,syh,syhy,bs_sy) // cost_function_acc(sy,syh,syhy,yl) // ...\n",
    "    gradient = [   ]       # [   ]: gradient_descent_fbeta(p,X,yh,y,bs,syh,syhy,bs_sy) // ...\n",
    "    theta = update_loss(theta,lr,gradient)\n",
    "outcome_r = predict(X_train_a,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a335cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b72402b2",
   "metadata": {},
   "source": [
    "## 2. Code in Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e44e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ MSE ################################\n",
    "def MSE(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.math.square(y_true - y_pred))\n",
    "\n",
    "################################ BCE ################################\n",
    "def BCE(y_true, y_pred):\n",
    "    return -tf.reduce_mean(y_true*tf.math.log(y_pred)+(1-y_true)*tf.math.log(1-y_pred))\n",
    "\n",
    "################################ Ours_Accu ################################\n",
    "def Ours_Accu(y_true, y_pred):\n",
    "    y_pred = 1/(1+tf.math.exp(-L*(y_pred-0.5)))\n",
    "    yl = y_train.shape[0]\n",
    "    accu = (yl-tf.reduce_sum(y_true)-tf.reduce_sum(y_pred)+2*tf.reduce_sum(y_true*y_pred)) / yl\n",
    "    return 1-accu\n",
    "\n",
    "################################ Ours_Fbeta ################################\n",
    "def Ours_Fbeta(y_true, y_pred):\n",
    "#     beta = 1 \n",
    "    y_pred = 1/(1+tf.math.exp(-L*(y_pred-0.5)))\n",
    "    numerator = (1+beta**2)*tf.reduce_sum(y_true*y_pred)\n",
    "    denominator = (beta**2)*tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)\n",
    "    return 1-(numerator/denominator)\n",
    "\n",
    "################################ Ours_Gmean ################################\n",
    "def Ours_Gmean(y_true, y_pred):\n",
    "    y_pred = 1/(1+tf.math.exp(-L*(y_pred-0.5)))\n",
    "    syhy = tf.reduce_sum(y_true*y_pred)\n",
    "    sy = tf.reduce_sum(y_true)\n",
    "    yl = (y_train.shape[0])\n",
    "#     gmean = syhy*(yl-tf.reduce_sum(y_pred)-sy+syhy)/(sy*(yl-sy))\n",
    "    gmean = tf.sqrt(syhy*(yl-tf.reduce_sum(y_pred)-sy+syhy)/(sy*(yl-sy)))\n",
    "    return 1-gmean\n",
    "\n",
    "################################ Ours_BAccu ################################\n",
    "def Ours_BAccu(y_true, y_pred):\n",
    "    y_pred = 1/(1+tf.math.exp(-L*(y_pred-0.5)))\n",
    "    syhy = tf.reduce_sum(y_true*y_pred)\n",
    "    sy = tf.reduce_sum(y_true)\n",
    "    yl = y_train.shape[0]\n",
    "    baccu = (yl*(syhy+sy)-sy*(tf.reduce_sum(y_pred)+sy)) / (2*sy*(yl-sy))\n",
    "    return 1-baccu\n",
    "\n",
    "hidden_node = 2\n",
    "kernel_initializer=keras.initializers.he_normal(seed=100)\n",
    "activation = 'sigmoid'\n",
    "learning_rate = 0.001\n",
    "batch_size = int(X.shape[0]*0.9 * 0.05)\n",
    "epochs=100\n",
    "L = 73\n",
    "threshold = 0.5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_node, input_dim=X.shape[1], kernel_initializer=kernel_initializer))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation))\n",
    "model.add(Dense(1))\n",
    "opt = optimizers.Adam(learning_rate = learning_rate)\n",
    "model.compile(loss=[  ], optimizer=opt, metrics=['accuracy']) # [  ]: MSE // BCE // Ours_Accu // Ours_Fbeta // ...\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, verbose=0, batch_size=batch_size)\n",
    "predicted = []\n",
    "result = model.predict(X_test)\n",
    "for i in range(X_test.shape[0]):\n",
    "    if result[i] <= threshold:\n",
    "        predicted.append(0)\n",
    "    else:\n",
    "        predicted.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d00205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b501e2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370ae78e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b329e69f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
